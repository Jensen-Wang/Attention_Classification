# Attention_Classification
* LSTM-attention 是在lstm的输出上使用attention机制
* self-attention 是修改Tranformer的源码，只利用其中Encoder部分来进行序列分类
* [分词文件和word2vec的词向量模型可以通过这个程序得到](https://github.com/huangqianfei0916/Fasta2svm/tree/master/Fasta2svm-1.0)
