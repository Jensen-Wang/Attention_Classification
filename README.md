# Attention_Classification
* LSTM-attention 是在lstm的输出上使用attention机制
* self-attention 是修改Tranformer的源码，只利用其中Encoder部分来进行序列分类
